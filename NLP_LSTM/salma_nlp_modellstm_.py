# -*- coding: utf-8 -*-
"""Salma_NLP_ModelLSTM_.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iA-ovb1FmbUIekX1ZO9LFcXMmv7lVSMj

*   **Nama : SALMA NATASYA AZZAHRA**
*   **No.Regis : 1494037162100-2431**
"""

from google.colab import drive
drive.mount('/content/drive')

# Import library and read datasets
import pandas as pd
import re
import tensorflow as tf
import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

df = pd.read_csv('https://raw.githubusercontent.com/rasyidev/well-known-datasets/main/juli2train.csv')
df.head()

df.tail()

#Checking for null values
df.isnull().sum()

# Cleaning text twitter
def preprocessor(data):
    corpus = []
    for i in range(len(data)):
        # remove urls
        tweet = re.sub(r'http\S+', ' ', data[i])
        # remove html tags
        tweet = re.sub(r'<.*?>', ' ', tweet)
        #remove @...
        tweet = re.sub('@\w+([-.]\w+)*', ' ', tweet)
        #remove &...
        tweet = re.sub('&\w+([-.]\w+)*', ' ', tweet)
        # remove digits
        tweet = re.sub(r'\d+', ' ', tweet)
        # remove hashtags
        tweet = re.sub(r'#\w+', ' ', tweet)
        review = re.sub('[^a-zA-Z]', ' ', tweet)
        review = review.lower()
        review = review.split()
        corpus.append(review)
    return corpus

df['CleanTweet'] = preprocessor(df['tweet'])
df

# View class variables using .unique()
df['label'].unique()

# Perform one-hot-encoding and create a new dataframe
label = pd.get_dummies(df.label)
df_new = pd.concat([df, label], axis=1)
df_new = df_new.drop(columns='label')
df_new

# Converts the values ​​from the dataframe into the numpy array data type using .values()
oritweet = df_new['CleanTweet'].values
sentimen = df_new[[1, 0]].values #1= positif 0= negatif

# Divide the data into training data and testing data
oritweet_latih, oritweet_test, sentimen_latih, sentimen_test = train_test_split(oritweet, sentimen, test_size=0.2)

# Convert each word in the dataset into a numeric number with the Tokenizer function
tokenizer = Tokenizer(num_words=10000, oov_token='<oov>')
tokenizer.fit_on_texts(oritweet_latih) 
tokenizer.fit_on_texts(oritweet_test)
 
sekuens_latih = tokenizer.texts_to_sequences(oritweet_latih)
sekuens_test = tokenizer.texts_to_sequences(oritweet_test)
 
padded_latih = pad_sequences(sekuens_latih,padding='post',
                             maxlen=20,truncating='post') 
padded_test = pad_sequences(sekuens_test,padding='post',
                             maxlen=20,truncating='post')

# Callback Function

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.95):
      print("\nAkurasi telah mencapai >90%!")
      self.model.stop_training = True
callbacks = myCallback()

# Creating a model architecture using layers Embedding and LSTM
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=10000, output_dim=16),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(2, activation='softmax') #dense 2 karena hanya ada 1 dan 0 pada label
])
model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

# Train the model by calling the function .fit()
num_epochs = 20
history = model.fit(padded_latih, sentimen_latih, epochs=num_epochs,validation_data=(padded_test, sentimen_test), verbose=2)

# plot loss and accuracy during training and validation.
# Plot Loss Training
import matplotlib.pyplot as plt
plt.plot(history.history['loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train'], loc='upper right')
plt.show()

# Plot Accuracy Training
plt.plot(history.history['accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train'], loc='lower right')
plt.show()